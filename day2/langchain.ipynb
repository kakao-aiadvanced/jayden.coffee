{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-30T07:07:44.665688Z",
     "start_time": "2024-07-30T07:07:42.002482Z"
    }
   },
   "source": [
    "%%capture --no-stderr\n",
    "%pip install langchain langchainhub langchain-openai langchain-openai langchain_chroma langchain-text-splitters langchain_community faiss-cpu"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T06:26:31.958885Z",
     "start_time": "2024-07-30T06:26:31.927885Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "# os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
    "os.environ['OPENAI_API_KEY'] = ''\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ],
   "id": "3455548d05bf3ba",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T06:29:50.448103Z",
     "start_time": "2024-07-30T06:29:49.724204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
    ").to_messages()\n",
    "\n",
    "example_messages"
   ],
   "id": "d6d8edbd7ed6b6cd",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T07:48:25.007670Z",
     "start_time": "2024-07-30T07:48:19.662901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Docs Retrieval\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "class RoutingResult(BaseModel):\n",
    "    relevance: str = Field(..., description=\"The relevance of the document to the question. It must be one of the following: 'Yes', 'No'\")\n",
    "    context: str = Field(..., description=\"The context of the document\")\n",
    "    question: str = Field(..., description=\"The question to be answered\")\n",
    "\n",
    "routing_parser = JsonOutputParser(pydantic_object=RoutingResult)\n",
    "routing_prompt = PromptTemplate(\n",
    "    template='''Given the question and context, determine if the question is relevant to the context. If it is, give relevance as 'Yes' If it is not, give relevance as 'No'.\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    {question}\n",
    "    \n",
    "    {format_instructions}\n",
    "    ''',\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": routing_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "routing_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | routing_prompt\n",
    "    | llm\n",
    "    | routing_parser\n",
    ")\n",
    "# routing_chain\n",
    "routing_chain.invoke(\"What is Task Decomposition?\")"
   ],
   "id": "1e9136928fb792fc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'relevance': 'Yes',\n",
       " 'context': 'Fig. 1. Overview of a LLM-powered autonomous agent system. Component One: Planning# A complicated task usually involves many steps. An agent needs to know what they are and plan ahead. Task Decomposition# Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.',\n",
       " 'question': 'What is Task Decomposition?'}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:00:47.415141Z",
     "start_time": "2024-07-30T08:00:38.383818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class AnswerResult(BaseModel):\n",
    "    answer: str = Field(..., description=\"The answer to the question\")\n",
    "    context: str = Field(..., description=\"The context of the document\")\n",
    "    question: str = Field(..., description=\"The question to be answered\")\n",
    "\n",
    "answer_parser = JsonOutputParser(pydantic_object=AnswerResult)\n",
    "\n",
    "answer_prompt = PromptTemplate(\n",
    "    template='''if relevance is Yes, Given the question and context, answer the question. Otherwise, give an error message.\n",
    "    {relevance}\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    {question}\n",
    "    \n",
    "    {format_instructions}\n",
    "    ''',\n",
    "    input_variables=[\"relevance\",\"context\",\"question\"],\n",
    "    partial_variables={\"format_instructions\": answer_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "answer_chain = routing_chain | answer_prompt | llm | answer_parser\n",
    "\n",
    "print(f'task: {answer_chain.invoke(\"What is Task Decomposition?\")}')\n",
    "print(f'game: {answer_chain.invoke(\"What is the Elden Rings?\")}')"
   ],
   "id": "4ff991fe4e6f318b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: {'answer': 'Task Decomposition is a technique in which a complicated task is broken down into smaller and simpler steps, making it more manageable. This process often involves using a chain of thought (CoT) prompting technique to enhance model performance on complex tasks by instructing the model to think step by step.', 'context': 'Fig. 1. Overview of a LLM-powered autonomous agent system. Component One: Planning# A complicated task usually involves many steps. An agent needs to know what they are and plan ahead. Task Decomposition# Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.', 'question': 'What is Task Decomposition?'}\n",
      "game: {'answer': 'Error: The relevance is No. The question about Elden Rings does not relate to the provided context about AlfWorld and Hallucination in experiments.', 'context': 'Fig. 4. Experiments on AlfWorld Env and HotpotQA. Hallucination is a more common failure than inefficient planning in AlfWorld. (Image source: Shinn & Labash, 2023)', 'question': 'What is the Elden Rings?'}\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-30T08:01:01.265909Z",
     "start_time": "2024-07-30T08:00:50.600212Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HallucinationResult(BaseModel):\n",
    "    hallucination: str = Field(..., description=\"The hallucination generated by the model\")\n",
    "    context: str = Field(..., description=\"The context of the document\")\n",
    "    question: str = Field(..., description=\"The question to be answered\")\n",
    "    answer: str = Field(..., description=\"The answer to the question\")\n",
    "\n",
    "hallucination_parser = JsonOutputParser(pydantic_object=HallucinationResult)\n",
    "\n",
    "hallucination_prompt = PromptTemplate(\n",
    "    template='''Given the question, context and answer, check if there is any hallucination in the answer. If there is, return hallucination as 'Yes'. Otherwise, return 'No'.\n",
    "    \n",
    "    {context}\n",
    "    \n",
    "    {question}\n",
    "    \n",
    "    {answer}\n",
    "    \n",
    "    {format_instructions}\n",
    "    ''',\n",
    "    input_variables=[\"context\",\"question\",\"answer\"],\n",
    "    partial_variables={\"format_instructions\": hallucination_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "hallucination_chain = answer_chain | hallucination_prompt | llm | hallucination_parser\n",
    "\n",
    "print(f'task: {hallucination_chain.invoke(\"What is Task Decomposition?\")}')"
   ],
   "id": "fd90f646be52600f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task: {'hallucination': 'No', 'context': \"Fig. 1. Overview of a LLM-powered autonomous agent system. Component One: Planning# A complicated task usually involves many steps. An agent needs to know what they are and plan ahead. Task Decomposition# Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to 'think step by step' to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\", 'question': 'What is Task Decomposition?', 'answer': 'Task Decomposition is a process in which a complicated task is broken down into smaller and simpler steps, allowing an agent to plan ahead and approach the task more manageably. This technique is often enhanced by Chain of Thought (CoT) prompting, which encourages the model to think step by step.'}\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "6660a4cad012712f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
